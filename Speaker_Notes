Final Project Speaker Notes

### Outline
- Introduction to our topic
    - Overview of S&P 500 and our selected stocks
    - Basics of S&P 500
    - Our hand picked stocks
- Reason why we selected our topic 
- Description of our source of data 
- Questions they hope to answer with the data 
- Description of Analysis
    - Description of the data exploration phase of the project
    - Description of the analysis phase of the project 
- Technologies, languages, tools, and algorithms used throughout the project 
- Result of analysis
    - Recommendation for future analysis 
    - Anything the team would have done differently
- Takeaways and Conclusion

# ROARK
Topic:
- Our selected topic entails using machine learning to predict future stock prices based on historical data. The machine learning model will take in stock data and will from the last three years and will output prediction prices of the S&P 500. Our dataset contains the monthly stock price of the S&P 500 from the start of 2019 along with 5 individual stocks and their respective sectors. Various machine learning models will analyze these relationships to better understand and predict stock performance.

Reason we selected the topic:
- We selected this topic due to our interest in the stock market and desire to create a machine learning model that could potentially predict market trends based on previous performance data. 

- We wanted to predict something relevant to the our economy. After analyzing various data sets, we decided we wanted to better understand the S&P 500. We each picked an individual sector with a stock in the   S&P 500 of interest to learn more about.

Background Info on S&P 500:
- The S&P 500 Index, or Standard & Poor's 500 Index, is a market-capitalization-weighted index of 500 leading publicly traded companies in the U.S. It is not an exact list of the top 500 U.S. companies by market cap because there are other criteria that the index includes. Still, the S&P 500 index is regarded as one of the best gauges of prominent American equities' performance and the stock market overall.

Description of the source of data:
- Daily starting and ending price of the S&P 500 from 2019.

- Daily prices for our handpicked stocks from 2019:
    (Apple, Nike, Kellogg, Occidental Petroleum, and C.H. Robinson)

- Daily prices for their individual sectors: 
(Tech, Consumer Discretionary, Consumer Staples, Energy, Industrials)


### Questions we hope to answer with the data:
- We hope to find out if it is possible to create a machine learning model that can accurately predict stock prices. Specifically, we want to find out if we can accurately predict the S&P 500 over a certain period of time. 

Question : How accurately can we predict stock prices based on our created machine learning models?

- Another question we want to answer is if there is a relationship between historical trends and future stock performance.

Question: Is there a relationship between historical trends and future stock performance?

# KEVIN
Database Integration

✓ Database stores static data for use during the project
✓ Database interfaces with the project in some format (e.g., scraping updates the database, or database connects to the model)
✓ Includes at least two tables (or collections, if using MongoDB)
✓ Includes at least one join using the database language (not including any joins in Pandas)
✓ Includes at least one connection string (using SQLAlchemy or PyMongo)

 #1.  Db stores static data that is taken at the end of the trading day, as opposed to streaming live updated data.

#2. Db will connect directly to the model, using a python code in Jupyter notebook.

#3. There are 11 tables total in the Db, that contain daily stock data between Jan 2019 and Dec 2021.

#4.  Each stock table is joined to a Sector table for output into the model.  Each of these joined tables is also joined again with the S&P table.

#5.  Not sure about a connection string...using postgreSQL for the SQL interface.

#6.  The ERD with relationships is provided for the 5 stock tables and related S&P table.

# NICOLE
Data Exploration and Analysis
✓ Description of data preprocessing
✓ Description of feature engineering and the feature selection, including the team's decision-making process
✓ Description of how data was split into training and testing sets
✓ Explanation of model choice, including limitations and benefits
✓ Explanation of changes in model choice (if changes occurred between the Segment 2 and Segment 3 deliverables)
✓ Description of how model was trained (or retrained, if they are using an existing model)
✓ Description and explanation of model’s confusion matrix, including final accuracy score
Additionally, the model obviously addresses the question or problem the team is solving.
The data was pulled in from Postgres sql to be read into a pandas data frame. 

The team decided on three predictive modeling techniques based on how well known and respected each model was. 

The data was split into training and testing sets with the first 80% of the data in the training set and the last 20% in the testing set. This was done in order of dates and could not be random due to time series limitations. 

The model was trained by using the training data and getting a sense for the seasonality. Then after looking at graphs it was decided to perform a log transformation on the data to get a better fit for the outcome. 

The models performed fairly and displayed an average prediction but the models were not able to follow seasonality as we would have liked. 

The model does predict stocks and can say that when the S&P goes up how the corresponding sector will react.

# JOSE
- Machine learning 
is a branch of artificial intelligence (AI) and computer science which focuses on the use of data and algorithms 
to imitate the way that we learn, and gradually improving its accuracy. Through the use of statistical methods, algorithms are trained 
to make classifications or predictions, uncovering key insights within our data mining project.

e.g.

import numpy as np: keeps our code more readable.

import pandas as pd: tells Phyton to bring the pandas data analysis library into our current environment.

import matplotlib.pyplot as plt: is importer into our namespace under the shorter name plt. The pyplot is where the plot () scatter(), 
and other commands live

from pandas.plotting import lag_plot: is a scatter plot for a time series and the same data lagged.

import datetime as dt: Return a string stating the day of the week corresponding to datetime dt .

from sklearn.linear_model import LinearRegression: Is one of the best statistical models that studies the relationship between a dependent 
variable (Y) with a given set of independent variables (X). The relationship is established with the help of fitting a best line. 
sklearn.linear_model.LinearRegression is the module used to implement linear regression.

from sklearn.model_selection import train_test_split: is a function in Sklearn model selection for splitting data arrays into two subsets: 
for training data and for testing data. With this function, we do not need to divide the dataset manually. By default, 
Sklearn train_test_split will make random partitions for the two subsets.

from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error: Array-like value defines weights used to average errors. 
‘raw_values’ : Returns a full set of errors in case of multioutput input. ‘uniform_average’ : 
Errors of all outputs are averaged with uniform weight.

from fbprophet import Prophet: is Facebook prophet which required requires a pandas DataFrame with two columns:

ds, for datestamp, is a datestamp column in a format expected by pandas.
y, a numeric column containing the measurement we wish to forecast.
import pmdarima as pm: (originally pyramid-arima, for the anagram of 'py' + 'arima') is a statistical library designed to fill the void 
in Python's time series analysis capabilities. This includes: The equivalent of R's auto.arima functionality. A collection of statistical 
tests of stationarity and seasonality. Time series utilities, such as differencing and inverse differencing.

The machine learning aspect will take in stock data from the last three years of the dataset and will output prediction prices. 
The code will run two different forecasting models (ARIMA and Facebook Prophet) and will produce graphs displaying the predictions to 
compare each of the models.
e.g.
* Modeling
arima_model = ARIMA(ch_robinson_train_data['ch_robinson'], order=(1,1,0))
arima_fitted = arima_model.fit()
* Facebook Prophet Model e.g.
prophet_data = pd.DataFrame()
prophet_data['y'] = df_ch_robinson_log['ch_robinson']
prophet_data['ds'] = industrial_sector_pd.index
- train and validation
prophet_train = prophet_data[:int(prophet_data.shape[0]*0.80)]
prophet_test = prophet_data[int(prophet_data.shape[0]*0.80):]
- fit the model
prophet_model = Prophet(interval_width=0.95)
prophet_model.fit(prophet_train)
- predictions
close_prices = prophet_model.make_future_dataframe(periods=212)
forecast = prophet_model.predict(close_prices
- Database Storage
Selecting the right data store for our requirements is a key design decision. Reason why we have decided to use PostgresSQL Database as an 
standard language for accessing and manipulating databases, which provides various benefits to our development, such as saving time by 
automating routine tasks, locating and fixing errors, taking advantage of intelligent support from the IDE, and increasing overall productivity.
- Data Cleaning and Analysis
In this process, we review, analyze, detect, modify, or remove “dirty” data to make our dataset “clean.”
The way we check for unique values is to use the Pandas DataFrame's .
Pandas: will be used to clean the data and perform an exploratory analysis. Further analysis is completed using Python.

Technologies Used
- Git Hub:
Is a provider of Internet hosting for software development and version control using Git. 
- Visual Studio Code:
Is a source-code editor that we used with a variety of programming languages, including Python. 
- Kaggle:
Allowed us to find our data sets for this project.
- Phyton 3:
Functions provide our program with more modularity and allow us to reuse a lot of code.
- SQL Database:
Is a database programming language for querying and editing information.
we are using the same ERD program used in class, the Quick DBD.
- Tableau:
Is an American interactive data visualization software that we are using to present our DASHBOARD Blue Print.
- Snipping Tool:
Is a screenshot utility included in windows Vista and later versions that can take screenshots in a variety of ways, which we use to create 
the storyboard for this project.
- Excel:
Is a spreadsheet program from Microsoft and a component of its Office product group for business applications.
- JSON File:
Is a file that stores simple data structures and objects in JavaScript Object Notation (JSON) format, which is a standard data interchange format.
It is primarily used for transmitting data between a web application and a server.
- index.html:
As its name suggests index.html consists of two words index and html. The index simply means shortlist for multiple items. In web terminology 
generally used to showcase the website pages, categories or parts on a single page.
- HTML:
Hypertext Markup Language, a standardized system for tagging text files to achieve font, color, graphic, and hyperlink effects on 
World Wide Web pages.

# NICOLE
- Analysis Phase and Results
Linear Regression
The purpose of viewing the linear regression model was to see how closely correlated each company’s stock or industry was related to the S&P (i.e. if the S&P increased would the company or industry also increase).

Arima Model
ARIMA: Autoregressive Integrated Moving Average. This is a statistical model that attempts to use past observations of the target variable to forecast its future values. Some limitations to the ARIMA model is that it has difficulty predicting turning points, it struggles with seasonality, and it performs well on short term forecasts but has poorer performance long term. This model was the second best and did a good job predicting an estimated average of where the stock prices would go. This model did not perform as well when it comes to seasonality/trend.

Facebook Prophet Model
The Facebook Prophet model is an additive regression model (like the ARIMA) but it includes growth trend and seasonal components. Some limitations of the Prophet model are it has a tendency to overfit the data, and it requires data to be in a specific format. Overall this model performed the best and was more accurate in terms on seasonality and trend. 

### Link to our Website


# ROARK
Recommendation for Future Analysis

Individual Stocks vs Individual Stock Sector
Use the same data set at a different time (i.e. 2010 - 2015)
Random Forest Classifier Algorithm

Since decision trees where not necessary in our project due to time consuming and the small size data set, Investing time on the creation of a Random forest Algorithms for large dataset might be beneficial on:
- The handle of thousands of input variables without variable deletion.
- Run efficiently on large datasets
- Can rank the importance of input variables in a natural way
- Are robust to aoutliers and nonlinear data
- Are robust against overfitting as all of those weak learners are trained on different pieces of data

Next Time
Anything we would have done differently?


### Presentation Slides
- Here is the link to our Google Slides Presentation:
https://docs.google.com/presentation/d/1LKO1Rd-QlpozrJrnYhYHzJuyGJvUBEsal7sHNvLBlyc/edit#slide=id.g134b22ff32b_0_37
